{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>USFWS_PINWR_PrePlot_Data_Prep_CalcSandWtPercents.ipynb</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core script in this notebook is designed to convert raw sand sample weights to bin (seive screen and pan) weight percentages and cumulative weight percentages, and record these to an output file for use in plotting and/or analysis. The raw samples come from sands collected along the ocean beach on the Pea Island National Wildlife Refuge. \n",
    "\n",
    "Processing proceeds by considering a single survey (sometimes referred to as a sampling). The surveys conducted to date along Pea Island include: '201407','201409','201504','201508','201510','201602','20604','201608'\n",
    "\n",
    "the script proceeds by converting each of as many as 5 sample sites: 'S1','S2','S3','S4','S5'\n",
    "\n",
    "along each of 27 transects: 'C11','C10','C9','C8','C7','C6','C5','C4','C3','C2','C1','T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12','T13','T14','T15','T16'\n",
    "\n",
    "for each sample, seived (binned) sands at   intervals the individual screen weight% and cumulative weight% are computed and these weight percentages are written to a comma separated text file. \n",
    "\n",
    "only weight percent is currently written to the Survey Plotting Data file\n",
    "\n",
    "The weight percentages can be used for plotting, or additional exploratory and explanatory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE: Use this notebook to prepare data to create stacked pastel, 'rainbow', and other plots. No actual plotting is performed here...__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the requisite Python libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing sample: S5 from transect C11\n",
      "Missing sample: S5 from transect C10\n",
      "Missing sample: S5 from transect C9\n",
      "Missing sample: S5 from transect C8\n",
      "Missing sample: S5 from transect C7\n",
      "Missing sample: S5 from transect C6\n",
      "Missing sample: S5 from transect C5\n",
      "Missing sample: S5 from transect C4\n",
      "Missing sample: S5 from transect C3\n",
      "Missing sample: S5 from transect C2\n",
      "Missing sample: S5 from transect C1\n",
      "Missing sample: S5 from transect T1\n",
      "Missing sample: S5 from transect T2\n",
      "Missing sample: S5 from transect T3\n",
      "Missing sample: S5 from transect T4\n",
      "Missing sample: S5 from transect T5\n",
      "Missing sample: S5 from transect T6\n",
      "Missing sample: S5 from transect T7\n",
      "Missing sample: S5 from transect T8\n",
      "Missing sample: S5 from transect T9\n",
      "Missing sample: S5 from transect T10\n",
      "Missing sample: S5 from transect T11\n",
      "Missing sample: S5 from transect T12\n",
      "Missing sample: S5 from transect T13\n",
      "Missing sample: S5 from transect T14\n",
      "Missing sample: S5 from transect T15\n",
      "Missing sample: S5 from transect T16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S4</th>\n",
       "      <th>weight_percent</th>\n",
       "      <th>cumWtPercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>phi_-1</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_-0.5</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_0</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.636</td>\n",
       "      <td>2.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_0.5</th>\n",
       "      <td>0.78</td>\n",
       "      <td>1.055</td>\n",
       "      <td>3.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_1</th>\n",
       "      <td>2.73</td>\n",
       "      <td>3.694</td>\n",
       "      <td>7.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_1.25</th>\n",
       "      <td>3.11</td>\n",
       "      <td>4.208</td>\n",
       "      <td>11.406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_1.5</th>\n",
       "      <td>6.52</td>\n",
       "      <td>8.822</td>\n",
       "      <td>20.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_1.75</th>\n",
       "      <td>8.44</td>\n",
       "      <td>11.419</td>\n",
       "      <td>31.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_2</th>\n",
       "      <td>13.87</td>\n",
       "      <td>18.766</td>\n",
       "      <td>50.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_2.5</th>\n",
       "      <td>14.51</td>\n",
       "      <td>19.632</td>\n",
       "      <td>70.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_3</th>\n",
       "      <td>13.76</td>\n",
       "      <td>18.617</td>\n",
       "      <td>88.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phi_3.5</th>\n",
       "      <td>8.16</td>\n",
       "      <td>11.040</td>\n",
       "      <td>99.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remainder</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.298</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              S4  weight_percent  cumWtPercent\n",
       "phi_-1      0.64           0.866         0.866\n",
       "phi_-0.5    0.70           0.947         1.813\n",
       "phi_0       0.47           0.636         2.449\n",
       "phi_0.5     0.78           1.055         3.504\n",
       "phi_1       2.73           3.694         7.198\n",
       "phi_1.25    3.11           4.208        11.406\n",
       "phi_1.5     6.52           8.822        20.228\n",
       "phi_1.75    8.44          11.419        31.647\n",
       "phi_2      13.87          18.766        50.413\n",
       "phi_2.5    14.51          19.632        70.045\n",
       "phi_3      13.76          18.617        88.662\n",
       "phi_3.5     8.16          11.040        99.702\n",
       "remainder   0.22           0.298       100.000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the path to and name of the raw sediment data file\n",
    "path='/Users/paulp/GoogleDrive/projects/USFWS_PeaIslandNWR/data/'\n",
    "fn='FWSGrainSizeAnalysis2015_10.csv'                ####### ADJUST #######\n",
    "\n",
    "# define the field (column) names in the raw sediment data file\n",
    "hdr=['sheet_code','sample_date','transect_id','sample_number','pan_weight','pan+wet_weight','pan+dry_weight','dry_weight','phi_-1','phi_-0.5','phi_0','phi_0.5','phi_1','phi_1.25','phi_1.5','phi_1.75','phi_2','phi_2.5','phi_3','phi_3.5','remainder','summed_weight']\n",
    "\n",
    "# the current survey (sampling) id, the tranects in the survey, and the samples collected along each transect\n",
    "survey='201510'                                     ####### ADJUST #######\n",
    "\n",
    "transects=['C11','C10','C9','C8','C7','C6','C5','C4','C3','C2','C1','T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12','T13','T14','T15','T16']\n",
    "\n",
    "samples=['S1','S2','S3','S4','S5']\n",
    "\n",
    "\n",
    "# open file to receive sample weight percents:\n",
    "fp=open(path+'SedSAS_Survey_Plotting_Data_'+survey+'.csv', 'w')\n",
    "\n",
    "# load the contents of the raw sediment data file into a pandas dataframe:\n",
    "df=pd.read_csv(path+fn, header=0, names=hdr)\n",
    "\n",
    "# for each sample location along each transect in the current survey, compute the individual bin (screen)\n",
    "# weight percentages and cumulative weight percentage and write these to the Survey Plotting Data text file:\n",
    "# NOTE: at the present time, we are not posting the cumulative result to the Plotting Data file:\n",
    "for transect in transects:\n",
    "    try:\n",
    "        for sample in samples:\n",
    "            try:\n",
    "                qs='transect_id == '+'\\\"'+transect+'\\\"'\n",
    "\n",
    "                df1=df.query(qs).T\n",
    "        \n",
    "                cols=['S'+str(s) for s in range(1, len(df1.columns)+1 )  ]\n",
    "                df1.columns=cols\n",
    "   \n",
    "                df2=df1[sample]\n",
    "                \n",
    "                # now, get the pan weights, convert the series to a dataframe, and convert \n",
    "                # the string type value field to floating point types:\n",
    "                df3=df2.loc['phi_-1':'remainder'].to_frame().astype(float)\n",
    "\n",
    "                # compute weight% and cumulative weight% for the sample\n",
    "                df3['weight_percent']=round((df3/df3.sum())*100, 3)  \n",
    "                df3['cumWtPercent']=round(np.cumsum(df3.weight_percent),3)\n",
    "                  \n",
    "                # write the weight percent as a string to the file: SedSAS_Survey_Plotting_Data_XXXXX\n",
    "                wts=transect+','+sample+','+survey+','\n",
    "                for wt in df3['weight_percent']:\n",
    "                    wts=wts+str(wt)+','\n",
    "                \n",
    "                wts=wts.rstrip(',')+'\\n'\n",
    "                fp.write(wts)\n",
    "    \n",
    "            except:\n",
    "                fp.write(transect+','+sample+','+survey+','+'\\n')  # empty record\n",
    "                print('Missing sample:', sample,'from transect',transect)\n",
    "    except:\n",
    "        fp.write(transect+','+sample+','+survey+','+'\\n')          # empty record\n",
    "        print('Could not find transect:', transect)\n",
    "        \n",
    "fp.close()\n",
    "df3"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "63px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
